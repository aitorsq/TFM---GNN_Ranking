{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./functions\")\n",
    "\n",
    "from utils import *\n",
    "from model_bet import *\n",
    "from layer import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating graphs for the first experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 15 ER graphs\n",
      "10/05/2023 16:35:04: Graph index:1/15\n",
      "10/05/2023 16:35:04: Graph index:2/15\n",
      "10/05/2023 16:35:04: Graph index:3/15\n",
      "10/05/2023 16:35:04: Graph index:4/15\n",
      "10/05/2023 16:35:04: Graph index:5/15\n",
      "10/05/2023 16:35:04: Graph index:6/15\n",
      "10/05/2023 16:35:04: Graph index:7/15\n",
      "10/05/2023 16:35:04: Graph index:8/15\n",
      "10/05/2023 16:35:04: Graph index:9/15\n",
      "10/05/2023 16:35:04: Graph index:10/15\n",
      "10/05/2023 16:35:04: Graph index:11/15\n",
      "10/05/2023 16:35:04: Graph index:12/15\n",
      "10/05/2023 16:35:04: Graph index:13/15\n",
      "10/05/2023 16:35:04: Graph index:14/15\n",
      "10/05/2023 16:35:04: Graph index:15/15\n",
      "Generating 15 SF graphs\n",
      "10/05/2023 16:35:04: Graph index:1/15\n",
      "10/05/2023 16:35:05: Graph index:2/15\n",
      "10/05/2023 16:35:05: Graph index:3/15\n",
      "10/05/2023 16:35:05: Graph index:4/15\n",
      "10/05/2023 16:35:05: Graph index:5/15\n",
      "10/05/2023 16:35:06: Graph index:6/15\n",
      "10/05/2023 16:35:06: Graph index:7/15\n",
      "10/05/2023 16:35:06: Graph index:8/15\n",
      "10/05/2023 16:35:06: Graph index:9/15\n",
      "10/05/2023 16:35:07: Graph index:10/15\n",
      "10/05/2023 16:35:07: Graph index:11/15\n",
      "10/05/2023 16:35:07: Graph index:12/15\n",
      "10/05/2023 16:35:08: Graph index:13/15\n",
      "10/05/2023 16:35:08: Graph index:14/15\n",
      "10/05/2023 16:35:08: Graph index:15/15\n",
      "Graphs saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 500,#5000,\n",
    "    \"max_nodes\": 1000,#10000,\n",
    "    \"num_of_graphs\": 15,\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#,\"GRP\"],\n",
    "    \"generation_seeds\": [10]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    for seed in param[\"generation_seeds\"]:\n",
    "        \n",
    "        random.seed(seed)\n",
    "        print(f\"Generating {param['num_of_graphs']} {graph_type} graphs\")\n",
    "        list_bet_data = list()\n",
    "        for i in range(param['num_of_graphs']):\n",
    "            print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "            g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "            \n",
    "            if nx.number_of_isolates(g_nx)>0:\n",
    "                g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "                g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "\n",
    "            g_nkit = nx2nkit(g_nx)\n",
    "            bet_dict = cal_exact_bet(g_nkit)\n",
    "            deg_dict = cal_exact_degree(g_nkit)\n",
    "            list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "        fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['max_nodes']}_{param['min_nodes']}_nodes_{seed}_genseed.pickle\"    \n",
    "\n",
    "        with open(fname_bet,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets of the generated graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing synthetic graphs performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import pandas as pd\n",
    "\n",
    "param = {\n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                                                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "\n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"generation_seed\":[],\n",
    "            \"splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for splitseed in param[\"split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for num_copies in param[\"num_copies\"]:\n",
    "\n",
    "                train_file = f\"{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "                #Load training data\n",
    "                print(f\"Loading data...\")\n",
    "                with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                    list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                model_size = bc_mat_train.shape[0]\n",
    "                assert model_size == param['adj_size']\n",
    "                \n",
    "                list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "                \n",
    "                for model_seed in param[\"model_seeds\"]:\n",
    "                    #Model parameters\n",
    "\n",
    "                    torch.manual_seed(model_seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                    model.to(device)\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                    print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                    for e in range(num_epoch):\n",
    "                        print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                        #to check test loss while training\n",
    "                        with torch.no_grad():\n",
    "                            r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                        Results[\"gtype_train\"].append(train_file)\n",
    "                        Results[\"generation_seed\"].append(genseed)\n",
    "                        Results[\"splilt_seed\"].append(splitseed)\n",
    "                        Results[\"copies\"].append(num_copies)\n",
    "                        Results[\"adj_size\"].append(model_size)\n",
    "                        Results[\"model_seed\"].append(model_seed)\n",
    "                        Results[\"epochs\"].append(e)\n",
    "                        Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                        Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                        df = pd.DataFrame.from_dict(Results)\n",
    "                        #df.to_csv(\"output_synthetic_graphs_performance.csv\")\n",
    "                        df.to_csv(\"./outputs/synthetic_graphs_performance_exp1.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets varying replication parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "    \n",
    "    \"num_copies\": [1,2,10,20,40],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "\n",
    "        with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "            list_data = pickle.load(fopen)\n",
    "\n",
    "        num_graph = len(list_data)\n",
    "        assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "    \n",
    "        for splitseed in param[\"split_seeds\"]:\n",
    "            \n",
    "            random.seed(splitseed)\n",
    "\n",
    "            #For test split\n",
    "            if param[\"num_test\"] > 0:\n",
    "                list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                    pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "                    \n",
    "            for num_copies in param[\"num_copies\"]:\n",
    "                \n",
    "                random.seed(splitseed)\n",
    "                \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    \n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing performance when varying the replication parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import pandas as pd\n",
    "\n",
    "param = {\n",
    "    \"graph_files\": [\"ER_15_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_15_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                                                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "\n",
    "    \"num_copies\": [1,2,10,20,40],\n",
    "    \"split_seeds\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"generation_seed\":[],\n",
    "            \"splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for splitseed in param[\"split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for num_copies in param[\"num_copies\"]:\n",
    "\n",
    "                train_file = f\"{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\"\n",
    "                #Load training data\n",
    "                print(f\"Loading data...\")\n",
    "                with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                    list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                model_size = bc_mat_train.shape[0]\n",
    "                assert model_size == param['adj_size']\n",
    "                \n",
    "                list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "                \n",
    "                for model_seed in param[\"model_seeds\"]:\n",
    "                    #Model parameters\n",
    "\n",
    "                    torch.manual_seed(model_seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                    model.to(device)\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                    print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                    for e in range(num_epoch):\n",
    "                        print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                        #to check test loss while training\n",
    "                        with torch.no_grad():\n",
    "                            r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                        Results[\"gtype_train\"].append(train_file)\n",
    "                        Results[\"generation_seed\"].append(genseed)\n",
    "                        Results[\"splilt_seed\"].append(splitseed)\n",
    "                        Results[\"copies\"].append(num_copies)\n",
    "                        Results[\"adj_size\"].append(model_size)\n",
    "                        Results[\"model_seed\"].append(model_seed)\n",
    "                        Results[\"epochs\"].append(e)\n",
    "                        Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                        Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                        df = pd.DataFrame.from_dict(Results)\n",
    "                        #df.to_csv(\"output_synthetic_graphs_performance.csv\")\n",
    "                        df.to_csv(\"./outputs/synthetic_graphs_performance_varying_replication_parameter.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate a set of 10 synthetic graphs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 ER graphs\n",
      "09/05/2023 17:20:50: Graph index:1/10\n",
      "09/05/2023 17:20:50: Graph index:2/10\n",
      "09/05/2023 17:20:50: Graph index:3/10\n",
      "09/05/2023 17:20:50: Graph index:4/10\n",
      "09/05/2023 17:20:50: Graph index:5/10\n",
      "09/05/2023 17:20:50: Graph index:6/10\n",
      "09/05/2023 17:20:50: Graph index:7/10\n",
      "09/05/2023 17:20:50: Graph index:8/10\n",
      "09/05/2023 17:20:50: Graph index:9/10\n",
      "09/05/2023 17:20:50: Graph index:10/10\n",
      "Generating 10 SF graphs\n",
      "09/05/2023 17:20:51: Graph index:1/10\n",
      "09/05/2023 17:20:51: Graph index:2/10\n",
      "09/05/2023 17:20:51: Graph index:3/10\n",
      "09/05/2023 17:20:51: Graph index:4/10\n",
      "09/05/2023 17:20:51: Graph index:5/10\n",
      "09/05/2023 17:20:52: Graph index:6/10\n",
      "09/05/2023 17:20:52: Graph index:7/10\n",
      "09/05/2023 17:20:52: Graph index:8/10\n",
      "09/05/2023 17:20:52: Graph index:9/10\n",
      "09/05/2023 17:20:53: Graph index:10/10\n",
      "Graphs saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 500,#5000,\n",
    "    \"max_nodes\": 1000,#10000,\n",
    "    \"num_of_graphs\": 10,\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#,\"GRP\"],\n",
    "    \"generation_seeds\": [10]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    for seed in param[\"generation_seeds\"]:\n",
    "        \n",
    "        random.seed(seed)\n",
    "        print(f\"Generating {param['num_of_graphs']} {graph_type} graphs\")\n",
    "        list_bet_data = list()\n",
    "        for i in range(param['num_of_graphs']):\n",
    "            print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "            g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "            \n",
    "            if nx.number_of_isolates(g_nx)>0:\n",
    "                g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "                g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "\n",
    "            g_nkit = nx2nkit(g_nx)\n",
    "            bet_dict = cal_exact_bet(g_nkit)\n",
    "            deg_dict = cal_exact_degree(g_nkit)\n",
    "            list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "        fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['max_nodes']}_{param['min_nodes']}_nodes_{seed}_genseed.pickle\"    \n",
    "\n",
    "        with open(fname_bet,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate a set of 5 synthetic training graphs for training using different random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 ER graphs\n",
      "09/05/2023 17:27:14: Graph index:1/5\n",
      "09/05/2023 17:27:14: Graph index:2/5\n",
      "09/05/2023 17:27:14: Graph index:3/5\n",
      "09/05/2023 17:27:14: Graph index:4/5\n",
      "09/05/2023 17:27:14: Graph index:5/5\n",
      "Generating 5 ER graphs\n",
      "09/05/2023 17:27:14: Graph index:1/5\n",
      "09/05/2023 17:27:14: Graph index:2/5\n",
      "09/05/2023 17:27:14: Graph index:3/5\n",
      "09/05/2023 17:27:14: Graph index:4/5\n",
      "09/05/2023 17:27:14: Graph index:5/5\n",
      "Generating 5 ER graphs\n",
      "09/05/2023 17:27:14: Graph index:1/5\n",
      "09/05/2023 17:27:14: Graph index:2/5\n",
      "09/05/2023 17:27:14: Graph index:3/5\n",
      "09/05/2023 17:27:14: Graph index:4/5\n",
      "09/05/2023 17:27:14: Graph index:5/5\n",
      "Generating 5 ER graphs\n",
      "09/05/2023 17:27:14: Graph index:1/5\n",
      "09/05/2023 17:27:14: Graph index:2/5\n",
      "09/05/2023 17:27:14: Graph index:3/5\n",
      "09/05/2023 17:27:14: Graph index:4/5\n",
      "09/05/2023 17:27:14: Graph index:5/5\n",
      "Generating 5 ER graphs\n",
      "09/05/2023 17:27:14: Graph index:1/5\n",
      "09/05/2023 17:27:14: Graph index:2/5\n",
      "09/05/2023 17:27:14: Graph index:3/5\n",
      "09/05/2023 17:27:14: Graph index:4/5\n",
      "09/05/2023 17:27:14: Graph index:5/5\n",
      "Generating 5 SF graphs\n",
      "09/05/2023 17:27:14: Graph index:1/5\n",
      "09/05/2023 17:27:14: Graph index:2/5\n",
      "09/05/2023 17:27:15: Graph index:3/5\n",
      "09/05/2023 17:27:15: Graph index:4/5\n",
      "09/05/2023 17:27:15: Graph index:5/5\n",
      "Generating 5 SF graphs\n",
      "09/05/2023 17:27:15: Graph index:1/5\n",
      "09/05/2023 17:27:16: Graph index:2/5\n",
      "09/05/2023 17:27:16: Graph index:3/5\n",
      "09/05/2023 17:27:16: Graph index:4/5\n",
      "09/05/2023 17:27:16: Graph index:5/5\n",
      "Generating 5 SF graphs\n",
      "09/05/2023 17:27:16: Graph index:1/5\n",
      "09/05/2023 17:27:17: Graph index:2/5\n",
      "09/05/2023 17:27:17: Graph index:3/5\n",
      "09/05/2023 17:27:17: Graph index:4/5\n",
      "09/05/2023 17:27:17: Graph index:5/5\n",
      "Generating 5 SF graphs\n",
      "09/05/2023 17:27:18: Graph index:1/5\n",
      "09/05/2023 17:27:18: Graph index:2/5\n",
      "09/05/2023 17:27:18: Graph index:3/5\n",
      "09/05/2023 17:27:18: Graph index:4/5\n",
      "09/05/2023 17:27:18: Graph index:5/5\n",
      "Generating 5 SF graphs\n",
      "09/05/2023 17:27:19: Graph index:1/5\n",
      "09/05/2023 17:27:19: Graph index:2/5\n",
      "09/05/2023 17:27:19: Graph index:3/5\n",
      "09/05/2023 17:27:19: Graph index:4/5\n",
      "09/05/2023 17:27:20: Graph index:5/5\n",
      "Graphs saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "param = {\n",
    "    \"min_nodes\": 500,#5000,\n",
    "    \"max_nodes\": 1000,#10000,\n",
    "    \"num_of_graphs\": 5,\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#,\"GRP\"],\n",
    "    \"generation_seeds\": [j for j in range(5)]\n",
    "}\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "\n",
    "    for seed in param[\"generation_seeds\"]:\n",
    "        \n",
    "        random.seed(seed)\n",
    "        print(f\"Generating {param['num_of_graphs']} {graph_type} graphs\")\n",
    "        list_bet_data = list()\n",
    "        for i in range(param['num_of_graphs']):\n",
    "            print(f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}: Graph index:{i+1}/{param['num_of_graphs']}\")\n",
    "            g_nx = create_graph(graph_type,param['min_nodes'],param['max_nodes'])\n",
    "            \n",
    "            if nx.number_of_isolates(g_nx)>0:\n",
    "                g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "                g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "\n",
    "            g_nkit = nx2nkit(g_nx)\n",
    "            bet_dict = cal_exact_bet(g_nkit)\n",
    "            deg_dict = cal_exact_degree(g_nkit)\n",
    "            list_bet_data.append([g_nx,bet_dict,deg_dict])\n",
    "\n",
    "        fname_bet = f\"./graphs/{graph_type}_{param['num_of_graphs']}_graphs_{param['max_nodes']}_{param['min_nodes']}_nodes_{seed}_genseed.pickle\"    \n",
    "\n",
    "        with open(fname_bet,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "print(\"Graphs saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate the test splilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_10_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_10_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [10],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [0],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 0,\n",
    "    \"num_test\" : 10,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate the splits for the training sets with different random seed generation and fixed split seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_5_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_5_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [j for j in range(5)],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [0],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 0,\n",
    "    \n",
    "}\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate the splits for the training sets with different split seed and a generation seed fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \n",
    "    \"graph_files\": [\"ER_5_graphs_1000_500_nodes\",#[\"ER_15_graphs_10000_5000_nodes\",\n",
    "                    \"SF_5_graphs_1000_500_nodes\"],# \"SF_15_graphs_10000_5000_nodes\",\n",
    "                    # \"GRP_15_graphs_10000_5000_nodes\"],\n",
    "    \"generation_seeds\": [0],\n",
    "    \n",
    "    \"num_copies\": [10],#[100],\n",
    "    \"split_seeds\": [j for j in range(5)],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 0,\n",
    "    \n",
    "}\n",
    "\n",
    "for graph_file in param[\"graph_files\"]:\n",
    "    for genseed in param[\"generation_seeds\"]:\n",
    "        for num_copies in param[\"num_copies\"]:\n",
    "            for splitseed in param[\"split_seeds\"]:\n",
    "        \n",
    "                with open(f\"./graphs/{graph_file}_{genseed}_genseed.pickle\",\"rb\") as fopen:\n",
    "                    list_data = pickle.load(fopen)\n",
    "\n",
    "                num_graph = len(list_data)\n",
    "                assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "            \n",
    "                #For training split\n",
    "                if param[\"num_train\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = num_copies, adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/train/{graph_file}_{genseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n",
    "\n",
    "                #For test split\n",
    "                if param[\"num_test\"] > 0:\n",
    "                    random.seed(splitseed)\n",
    "                    list_graph, list_n_sequence, list_node_num, cent_mat, deg_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=param[\"adj_size\"])\n",
    "\n",
    "                    with open(f\"./data_splits/test/{graph_file}_{genseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{splitseed}_splitseed.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat, deg_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We train and test using the diferent graphs created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 graphs...\n",
      "Loading data...\n",
      "Processing 50 graphs...\n",
      "Training, total Number of epoches: 15\n",
      "Epoch number: 1/15\n",
      "   Average KT score on test graphs is: 0.8099276928089235 and std: 0.13012060688295063\n",
      "Epoch number: 2/15\n",
      "   Average KT score on test graphs is: 0.8929695437952889 and std: 0.09896723947733078\n",
      "Epoch number: 3/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/f5lz4rj56ws3v2hlc19c60t80000gn/T/ipykernel_27593/1415785793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch number: {e+1}/{num_epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_adj_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_adj_t_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_num_node_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc_mat_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                 \u001b[0;31m#to check test loss while training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tfm/tfm-GNN-Ranking/delivery/functions/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(list_adj_train, list_adj_t_train, list_num_node_train, bc_mat_train, model, device, optimizer, size)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0my_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0mtrue_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbc_mat_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtrue_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tfm/tfm-GNN-Ranking/delivery/functions/model_bet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, adj1, adj2)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#Score Calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mscore1_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mscore1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mscore1_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tfm/tfm-GNN-Ranking/delivery/functions/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_vec, dropout)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mscore_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mscore_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mscore_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generation seeds\n",
    "\n",
    "param = {\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#[\"ER\",\"SF\",\"GRP\"],\n",
    "    \"graphs_sizes\": \"1000_500_nodes\",\n",
    "    \"test_generation_seeds\": [10],\n",
    "    \"train_generation_seeds\": [j for j in range(5)],\n",
    "    \"test_split_seeds\": [0],\n",
    "    \"train_split_seeds\": [0],\n",
    "    \"num_copies\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"train_generation_seed\": [],\n",
    "            \"train_splilt_seed\": [],\n",
    "            \"test_generation_seed\": [],\n",
    "            \"test_splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "    for testgenseed in param[\"test_generation_seeds\"]:\n",
    "        for testsplitseed in param[\"test_split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_type}_{param['num_test']}_graphs_{param['graphs_sizes']}_{testgenseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{testsplitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for traingenseed in param[\"train_generation_seeds\"]:\n",
    "                for trainsplitseed in param[\"train_split_seeds\"]:\n",
    "                    for num_copies in param[\"num_copies\"]:\n",
    "                        \n",
    "                        train_file = f\"{graph_type}_{param['num_train']}_graphs_{param['graphs_sizes']}_{traingenseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{trainsplitseed}_splitseed.pickle\"\n",
    "\n",
    "                        #Load training data\n",
    "                        print(f\"Loading data...\")\n",
    "                        with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "\n",
    "                        model_size = bc_mat_train.shape[0]\n",
    "                        assert model_size == param['adj_size']\n",
    "                        \n",
    "                        for model_seed in param[\"model_seeds\"]:\n",
    "                            #Model parameters\n",
    "\n",
    "                            torch.manual_seed(model_seed)\n",
    "                            \n",
    "                            hidden = 20\n",
    "                            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                            model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                            model.to(device)\n",
    "\n",
    "                            optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                            num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                            print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                            for e in range(num_epoch):\n",
    "                                print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                                train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                                #to check test loss while training\n",
    "                                with torch.no_grad():\n",
    "                                    r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                                Results[\"gtype_train\"].append(train_file)\n",
    "                                Results[\"train_generation_seed\"].append(traingenseed)\n",
    "                                Results[\"train_splilt_seed\"].append(trainsplitseed)\n",
    "                                Results[\"test_generation_seed\"].append(testgenseed)\n",
    "                                Results[\"test_splilt_seed\"].append(testsplitseed)\n",
    "                                Results[\"copies\"].append(num_copies)\n",
    "                                Results[\"adj_size\"].append(model_size)\n",
    "                                Results[\"model_seed\"].append(model_seed)\n",
    "                                Results[\"epochs\"].append(e)\n",
    "                                Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                                Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                                df = pd.DataFrame.from_dict(Results)\n",
    "                                df.to_csv(\"./outputs/synthetic_graphs_performance_variating_generation_random_seeds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 graphs...\n",
      "Loading data...\n",
      "Processing 50 graphs...\n",
      "Training, total Number of epoches: 15\n",
      "Epoch number: 1/15\n",
      "   Average KT score on test graphs is: 0.8099276928089235 and std: 0.13012060688295063\n",
      "Epoch number: 2/15\n",
      "   Average KT score on test graphs is: 0.8929695437952889 and std: 0.09896723947733078\n",
      "Epoch number: 3/15\n",
      "   Average KT score on test graphs is: 0.9085107088621435 and std: 0.08646247414480582\n",
      "Epoch number: 4/15\n",
      "   Average KT score on test graphs is: 0.9161735232220881 and std: 0.08074682582073266\n",
      "Epoch number: 5/15\n",
      "   Average KT score on test graphs is: 0.9207177496129187 and std: 0.07809144716302009\n",
      "Epoch number: 6/15\n",
      "   Average KT score on test graphs is: 0.923479337892368 and std: 0.07615157455455251\n",
      "Epoch number: 7/15\n",
      "   Average KT score on test graphs is: 0.9254569570570915 and std: 0.07535052425174125\n",
      "Epoch number: 8/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/f5lz4rj56ws3v2hlc19c60t80000gn/T/ipykernel_27593/3700711924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch number: {e+1}/{num_epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_adj_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_adj_t_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_num_node_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc_mat_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                 \u001b[0;31m#to check test loss while training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tfm/tfm-GNN-Ranking/delivery/functions/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(list_adj_train, list_adj_t_train, list_num_node_train, bc_mat_train, model, device, optimizer, size)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mloss_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_cal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mloss_rank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# replication seeds\n",
    "\n",
    "param = {\n",
    "    \"graph_types\": [\"ER\",\"SF\"],#[\"ER\",\"SF\",\"GRP\"],\n",
    "    \"graphs_sizes\": \"1000_500_nodes\",\n",
    "    \"test_generation_seeds\": [10],\n",
    "    \"train_generation_seeds\": [0],\n",
    "    \"test_split_seeds\": [0],\n",
    "    \"train_split_seeds\": [j for j in range(5)],\n",
    "    \"num_copies\": [10],\n",
    "    \"adj_size\" : 10000,\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \"model_seeds\": [15],\n",
    "    \"num_epochs\": 15,\n",
    "}\n",
    "\n",
    "Results = { \"gtype_train\":[],\n",
    "            \"train_generation_seed\": [],\n",
    "            \"train_splilt_seed\": [],\n",
    "            \"test_generation_seed\": [],\n",
    "            \"test_splilt_seed\": [],\n",
    "            \"copies\":[],\n",
    "            \"adj_size\": [],\n",
    "            \"model_seed\": [],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"std\":[]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for graph_type in param[\"graph_types\"]:\n",
    "    for testgenseed in param[\"test_generation_seeds\"]:\n",
    "        for testsplitseed in param[\"test_split_seeds\"]:\n",
    "            \n",
    "            test_file = f\"{graph_type}_{param['num_test']}_graphs_{param['graphs_sizes']}_{testgenseed}_genseed_{param['num_test']}_test_{param['adj_size']}_size_{testsplitseed}_splitseed.pickle\"\n",
    "            #Load test data\n",
    "            with open(\"./data_splits/test/\"+test_file,\"rb\") as fopen:\n",
    "                list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test,deg_mat_test = pickle.load(fopen)\n",
    "\n",
    "            list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,param['adj_size'])\n",
    "\n",
    "            for traingenseed in param[\"train_generation_seeds\"]:\n",
    "                for trainsplitseed in param[\"train_split_seeds\"]:\n",
    "                    for num_copies in param[\"num_copies\"]:\n",
    "                        \n",
    "                        train_file = f\"{graph_type}_{param['num_train']}_graphs_{param['graphs_sizes']}_{traingenseed}_genseed_{param['num_train']}_train_{num_copies}_copies_{param['adj_size']}_size_{trainsplitseed}_splitseed.pickle\"\n",
    "\n",
    "                        #Load training data\n",
    "                        print(f\"Loading data...\")\n",
    "                        with open(\"./data_splits/train/\"+train_file,\"rb\") as fopen:\n",
    "                            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train,deg_mat_train = pickle.load(fopen)\n",
    "\n",
    "                        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,param['adj_size'])\n",
    "\n",
    "                        model_size = bc_mat_train.shape[0]\n",
    "                        assert model_size == param['adj_size']\n",
    "                        \n",
    "                        for model_seed in param[\"model_seeds\"]:\n",
    "                            #Model parameters\n",
    "\n",
    "                            torch.manual_seed(model_seed)\n",
    "                            \n",
    "                            hidden = 20\n",
    "                            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                            model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "                            model.to(device)\n",
    "\n",
    "                            optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                            num_epoch = param[\"num_epochs\"]\n",
    "\n",
    "                            print(f\"Training, total Number of epoches: {num_epoch}\")\n",
    "                            for e in range(num_epoch):\n",
    "                                print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "                                train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model,device,optimizer,model_size)\n",
    "\n",
    "                                #to check test loss while training\n",
    "                                with torch.no_grad():\n",
    "                                    r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,deg_mat_test,model,device,model_size)\n",
    "\n",
    "                                Results[\"gtype_train\"].append(train_file)\n",
    "                                Results[\"train_generation_seed\"].append(traingenseed)\n",
    "                                Results[\"train_splilt_seed\"].append(trainsplitseed)\n",
    "                                Results[\"test_generation_seed\"].append(testgenseed)\n",
    "                                Results[\"test_splilt_seed\"].append(testsplitseed)\n",
    "                                Results[\"copies\"].append(num_copies)\n",
    "                                Results[\"adj_size\"].append(model_size)\n",
    "                                Results[\"model_seed\"].append(model_seed)\n",
    "                                Results[\"epochs\"].append(e)\n",
    "                                Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "                                Results[\"std\"].append(r[\"std\"])\n",
    "\n",
    "                                df = pd.DataFrame.from_dict(Results)\n",
    "                                df.to_csv(\"./outputs/synthetic_graphs_performance_variating_replication_random_seeds.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "836981034a4078c9f81aa3bbf2605e6a2991c189feb0614c725b1b8d5991d7f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
