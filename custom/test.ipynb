{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs: 5/5\n",
      "Processing graphs: 50/50\n",
      "Processing graphs: 100/100\n",
      "Processing graphs: 200/200\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/f5lz4rj56ws3v2hlc19c60t80000gn/T/ipykernel_61181/2865061579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "graphs = ['1-wiki-Vote','2-soc-Epinions','3-email-EuAll','4-web-Google']\n",
    "sizes = [10000]#,100000,300000,900000]\n",
    "out_input = []\n",
    "for i in range(len(graphs)):\n",
    "\n",
    "    g = graphs[i]\n",
    "    size = sizes[i]\n",
    "\n",
    "    for c in [1,10,20,40]:\n",
    "\n",
    "        data_train = f\"LFR_5_graphs_{c}_copies_{size}_size.pickle\"\n",
    "        \n",
    "        #Load training data\n",
    "        with open(\"./data_splits/train/\"+data_train,\"rb\") as fopen:\n",
    "            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train = pickle.load(fopen)\n",
    "\n",
    "        #Get adjacency matrices from graphs\n",
    "        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,size)\n",
    "        \n",
    "        out_input.append({\"data\": data_train,\"size\":size,\"c\":c,\"list_adj_train\":list_adj_train,\"list_adj_t_train\":list_adj_t_train})\n",
    "\n",
    "with open(\"google_colab/google_data.pickle\",'wb') as fopen:\n",
    "    pickle.dump(out_input,fopen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 410), (0, 4735), (0, 1632), (0, 2772), (1, 8495), (1, 1900), (1, 1163), (1, 6353), (1, 218), (1, 7428)]\n",
      "[(0, 5809), (0, 8661), (0, 7938), (0, 2245), (0, 406), (0, 7715), (0, 2176), (0, 9272), (0, 4837), (0, 4504)]\n",
      "[(0, 4425), (0, 4984), (0, 3461), (0, 3344), (1, 1590), (1, 7468), (1, 3162), (1, 1345), (1, 4711), (1, 1680)]\n",
      "[(0, 3182), (0, 571), (0, 4955), (0, 4912), (0, 3432), (1, 8083), (1, 1), (1, 875), (1, 2282), (2, 9544)]\n",
      "[(0, 6401), (0, 8652), (0, 569), (0, 5129), (0, 9484), (0, 635), (0, 1715), (0, 5860), (0, 2719), (0, 7865)]\n",
      "[(0, 3994), (0, 4972), (0, 7654), (0, 5287), (0, 7735), (0, 4880), (0, 4369), (1, 3667), (1, 1593), (1, 335)]\n",
      "[(0, 0), (0, 3227), (0, 7731), (0, 8465), (0, 6324), (0, 470), (1, 9564), (1, 4483), (1, 8552), (1, 3391)]\n",
      "[(0, 8055), (0, 1285), (0, 0), (0, 5727), (0, 4229), (0, 4316), (1, 5493), (1, 9040), (1, 4951), (1, 5713)]\n",
      "[(0, 7665), (0, 9971), (0, 822), (0, 2239), (0, 6979), (0, 8529), (0, 6138), (1, 473), (1, 1677), (1, 8528)]\n",
      "[(0, 2824), (0, 5219), (0, 8825), (0, 9495), (0, 9011), (0, 3412), (1, 1205), (1, 2484), (1, 10), (1, 3260)]\n",
      "[(0, 2468), (0, 3168), (0, 4386), (0, 9221), (0, 5174), (0, 537), (0, 7865), (1, 9985), (1, 3917), (1, 5642)]\n",
      "[(0, 7774), (0, 5577), (0, 5094), (0, 6656), (1, 6749), (1, 890), (1, 7262), (1, 8971), (1, 2600), (1, 1246)]\n",
      "[(0, 3581), (0, 4648), (0, 4013), (0, 3638), (0, 9832), (0, 3837), (1, 9797), (1, 552), (1, 1949), (1, 6785)]\n",
      "[(0, 623), (0, 8844), (0, 8705), (0, 1789), (0, 9608), (0, 1495), (0, 6253), (0, 4471), (0, 5728), (0, 867)]\n",
      "[(0, 3096), (0, 7897), (0, 5831), (0, 7359), (0, 196), (0, 0), (0, 3784), (0, 4816), (0, 990), (0, 8824)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model_bet import GNN_Bet\n",
    "import torch\n",
    "\n",
    "with open(\"google_data.pickle\",'rb') as fopen:\n",
    "    train_data = pickle.load(fopen)\n",
    "\n",
    "for trainset in train_data:\n",
    "    for seed in 10:\n",
    "\n",
    "        list_adj_train = trainset[\"list_adj_train\"]\n",
    "        list_adj_t_train = [\"list_adj_t_train\"]\n",
    "        model_size= trainset[\"size\"]\n",
    "        copies = trainset[\"c\"]\n",
    "        seed = 1\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        saving_path = f'LFR_5_graphs_{copies}_copies_{size}_size_{seed}_seed'\n",
    "            \n",
    "        #Model parameters\n",
    "        hidden = 20\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = GNN_Bet(ninput=model_size,nhid=hidden,dropout=0.6)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "\n",
    "        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model=model,device=device,optimizer=optimizer,size=size)\n",
    "        \n",
    "        torch.save(model.state_dict(), saving_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "836981034a4078c9f81aa3bbf2605e6a2991c189feb0614c725b1b8d5991d7f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
