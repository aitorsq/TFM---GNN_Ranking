{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFR generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkit import *\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from datetime import datetime\n",
    "\n",
    "def nx2nkit(g_nx):\n",
    "    \n",
    "    node_num = g_nx.number_of_nodes()\n",
    "    g_nkit = Graph(directed=True)\n",
    "    \n",
    "    for i in range(node_num):\n",
    "        g_nkit.addNode()\n",
    "    \n",
    "    for e1,e2 in g_nx.edges():\n",
    "        g_nkit.addEdge(e1,e2)\n",
    "        \n",
    "    assert g_nx.number_of_nodes()==g_nkit.numberOfNodes(),\"Number of nodes not matching\"\n",
    "    assert g_nx.number_of_edges()==g_nkit.numberOfEdges(),\"Number of edges not matching\"\n",
    "        \n",
    "    return g_nkit\n",
    "\n",
    "def cal_exact_bet(g_nkit):\n",
    "\n",
    "    #exact_bet = nx.betweenness_centrality(g_nx,normalized=True)\n",
    "\n",
    "    exact_bet = centrality.Betweenness(g_nkit,normalized=True).run().ranking()\n",
    "    exact_bet_dict = dict()\n",
    "    for j in exact_bet:\n",
    "        exact_bet_dict[j[0]] = j[1]\n",
    "    return exact_bet_dict\n",
    "\n",
    "\n",
    "def generate_bet_LFR_data(num_of_graphs,output_path):\n",
    "    \n",
    "    list_bet_data = list()\n",
    "\n",
    "    for i in range(num_of_graphs):\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print(f\"Graph index:{i+1}/{num_of_graphs}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "                g_nx = LFR_benchmark_graph(n=10000,tau1=3,tau2=1.5,mu=0.05,average_degree=6,min_community=20)\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        print(\"removing isolates\")\n",
    "        \n",
    "        if nx.number_of_isolates(g_nx)>0:\n",
    "            g_nx.remove_nodes_from(list(nx.isolates(g_nx)))\n",
    "        \n",
    "        g_nx = nx.convert_node_labels_to_integers(g_nx)\n",
    "        g_nkit = nx2nkit(g_nx)\n",
    "        bet_dict = cal_exact_bet(g_nkit)\n",
    "        list_bet_data.append([g_nx,bet_dict])\n",
    "\n",
    "        with open(f\"custom/\"+output_path,\"wb\") as fopen:\n",
    "            pickle.dump(list_bet_data,fopen)\n",
    "\n",
    "\n",
    "# Create train graphs and save them to sf_train_50.pickle\n",
    "num_of_graphs = 15\n",
    "\n",
    "output_path = f\"graphs/LFR_{num_of_graphs}_graphs.pickle\"\n",
    "\n",
    "generate_bet_LFR_data(num_of_graphs,output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create datasets LFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "param = {\n",
    "    \"size\" : [10000,100000,300000,900000],\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 10,\n",
    "    \"num_copies\": [1,10,20,40]\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"./graphs/LFR_15_graphs.pickle\",\"rb\") as fopen:\n",
    "    list_data = pickle.load(fopen)\n",
    "\n",
    "num_graph = len(list_data)\n",
    "assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "\n",
    "for size in param[\"size\"]:\n",
    "    for c in param[\"num_copies\"]:\n",
    "\n",
    "        #For training split\n",
    "        if param[\"num_train\"] > 0:\n",
    "            list_graph, list_n_sequence, list_node_num, cent_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = c,adj_size=size)\n",
    "\n",
    "            with open(f\"./data_splits/train/LFR_5_graphs_{c}_copies_{size}_size.pickle\",\"wb\") as fopen:\n",
    "                pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat],fopen)\n",
    "\n",
    "#For test split\n",
    "size = param[\"size\"][0]\n",
    "if param[\"num_test\"] > 0:\n",
    "    list_graph, list_n_sequence, list_node_num, cent_mat = create_dataset(list_data[param[\"num_train\"]:param[\"num_train\"]+param[\"num_test\"]],num_copies = 1,adj_size=size)\n",
    "\n",
    "    with open(f\"./data_splits/test/LFR_10_graphs_{size}_size.pickle\",\"wb\") as fopen:\n",
    "        pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat],fopen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create datasets scale free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "param = {\n",
    "    \"size\" : [10000,100000,300000,900000],\n",
    "    \"num_train\" : 5,\n",
    "    \"num_test\" : 0,\n",
    "    \"num_copies\": [1,10,20,40]\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"./graphs/SF_5_graphs_10000_nodes.pickle\",\"rb\") as fopen:\n",
    "    list_data = pickle.load(fopen)\n",
    "\n",
    "num_graph = len(list_data)\n",
    "assert param[\"num_train\"]+param[\"num_test\"] == num_graph,\"Required split size doesn't match number of graphs in pickle file.\"\n",
    "\n",
    "for size in param[\"size\"]:\n",
    "    for c in param[\"num_copies\"]:\n",
    "\n",
    "        #For training split\n",
    "        if param[\"num_train\"] > 0:\n",
    "            list_graph, list_n_sequence, list_node_num, cent_mat = create_dataset(list_data[:param[\"num_train\"]],num_copies = c,adj_size=size)\n",
    "\n",
    "            with open(f\"./data_splits/train/SF_5_graphs_10000_nodes_{c}_copies_{size}_size.pickle\",\"wb\") as fopen:\n",
    "                pickle.dump([list_graph,list_n_sequence,list_node_num,cent_mat],fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lfr train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "torch.manual_seed(15)\n",
    "\n",
    "\n",
    "param = {\n",
    "    \"size\" : [10000,100000,300000,900000],\n",
    "    \"num_copies\": [1,10,20,40]\n",
    "}\n",
    "\n",
    "#data_test = f\"LFR_10_graphs_10000_size.pickle\"\n",
    "##Load test data\n",
    "#with open(\"./data_splits/test/\"+data_test,\"rb\") as fopen:\n",
    "#    list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test = pickle.load(fopen)\n",
    "#\n",
    "#list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "for size in param[\"size\"]:\n",
    "    for c in param[\"num_copies\"]:\n",
    "\n",
    "        torch.manual_seed(15)\n",
    "        data_train = f\"LFR_5_graphs_{c}_copies_{size}_size.pickle\"    \n",
    "\n",
    "        #Load training data\n",
    "        print(f\"Loading data...\")\n",
    "        with open(\"./data_splits/train/\"+data_train,\"rb\") as fopen:\n",
    "            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train = pickle.load(fopen)\n",
    "\n",
    "        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,size)\n",
    "\n",
    "        #Model parameters\n",
    "        hidden = 20\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "        num_epoch = 15\n",
    "\n",
    "        for e in range(num_epoch):\n",
    "            print(f\"{c}_copies_{size}_size_{e}_epoch_{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "            train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train)\n",
    "            \n",
    "            saving_path = f\"./models/LFR/LFR_5_graphs_{c}_copies_{size}_size_{e}_epoch\"\n",
    "            torch.save(model.state_dict(), saving_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lfr performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "torch.manual_seed(15)\n",
    "\n",
    "\n",
    "size = 10000\n",
    "\n",
    "Results = {\"graph\":[],\n",
    "            \"size\": [],\n",
    "            \"copies\":[],\n",
    "            \"epochs\": [],\n",
    "            \"kendalltau\":[],\n",
    "            \"avg\":[]}\n",
    "\n",
    "data_path = f'LFR_10_graphs_10000_size.pickle'\n",
    "\n",
    "#Load test data\n",
    "with open(\"./data_splits/test/\"+data_path,\"rb\") as fopen:\n",
    "    list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test = pickle.load(fopen)\n",
    "\n",
    "#Get adjacency matrices from graphs\n",
    "#print(f\"Graphs to adjacency conversion.\")\n",
    "\n",
    "list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "\n",
    "for c in [1,10,20,40]:\n",
    "    for e in range(15):\n",
    "        \n",
    "        print(f\"copies: {c}, epoch {e}\")\n",
    "        \n",
    "        model_path = f\"./models/LFR/LFR_5_graphs_{c}_copies_{size}_size_{e}_epoch\"\n",
    "\n",
    "        #Model parameters\n",
    "        hidden = 20\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            r = test(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "        \n",
    "        Results[\"graph\"].append(\"LFR_10_graphs_10000_size\")\n",
    "        Results[\"size\"].append(size)\n",
    "        Results[\"copies\"].append(c)\n",
    "        Results[\"epochs\"].append(e)\n",
    "        Results[\"kendalltau\"].append(r[\"kt\"])\n",
    "        Results[\"avg\"].append(r[\"avg\"])\n",
    "\n",
    "\n",
    "        df = pd.DataFrame.from_dict(Results)\n",
    "        df.to_csv(\"output_LFR_graphs_peformance.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lfr performance real graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "graphs = ['1-wiki-Vote','2-soc-Epinions','3-email-EuAll','4-web-Google']\n",
    "sizes = [10000,100000,300000,900000]\n",
    "\n",
    "Results = {}\n",
    "\n",
    "for i in range(len(graphs)):\n",
    "\n",
    "    g = graphs[i]\n",
    "    size = sizes[i]\n",
    "    data_test = f'{g}_{size}_size.pickle'\n",
    "\n",
    "    Results[data_test] = {'true': [],'pred': []}\n",
    "    #Load test data\n",
    "    with open(\"./data_splits/test/\"+data_test,\"rb\") as fopen:\n",
    "        list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test = pickle.load(fopen)\n",
    "    list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "    for c in [1,10,20,40]:\n",
    "        \n",
    "        data_train = f\"LFR_5_graphs_{c}_copies_{size}_size.pickle\"\n",
    "        \n",
    "        #Load training data\n",
    "        with open(\"./data_splits/train/\"+data_train,\"rb\") as fopen:\n",
    "            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train = pickle.load(fopen)\n",
    "\n",
    "        #Get adjacency matrices from graphs\n",
    "        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,size)\n",
    "\n",
    "        for seed in range(10):\n",
    "            \n",
    "            currentresult = {'data_train': data_train, 'seed':seed, 'copies': c}\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            print(f\"G:{g}, size: {size}, copies: {c}, seed {seed}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "            \n",
    "            hidden = 20\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "\n",
    "#            with torch.no_grad():\n",
    "#                r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "#\n",
    "#            currentresult['no_train'] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "            train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model=model,device=device,optimizer=optimizer,size=size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "            currentresult['train'] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "            if len(Results[data_test]['true']) == 0:\n",
    "                Results[data_test]['true'] = r['true']\n",
    "\n",
    "            Results[data_test]['pred'].append(currentresult)\n",
    "\n",
    "            with open(\"LFR_real_performance.pickle\",\"wb\") as fopen:\n",
    "                pickle.dump(Results,fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sf performance real graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "graphs = ['1-wiki-Vote','2-soc-Epinions','3-email-EuAll','4-web-Google']\n",
    "sizes = [10000,100000,300000,900000]\n",
    "epochs = 5\n",
    "\n",
    "Results = {}\n",
    "\n",
    "for i in range(len(graphs)):\n",
    "\n",
    "    g = graphs[i]\n",
    "    size = sizes[i]\n",
    "    data_test = f'{g}_{size}_size.pickle'\n",
    "\n",
    "    Results[data_test] = {'true': [],'pred': []}\n",
    "    #Load test data\n",
    "    with open(\"./data_splits/test/\"+data_test,\"rb\") as fopen:\n",
    "        list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test = pickle.load(fopen)\n",
    "    list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "    for c in [1,10,20,40]:        \n",
    "        \n",
    "        data_train = f\"SF_5_graphs_10000_nodes_{c}_copies_{size}_size.pickle\"\n",
    "        \n",
    "        #Load training data\n",
    "        with open(\"./data_splits/train/\"+data_train,\"rb\") as fopen:\n",
    "            list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train = pickle.load(fopen)\n",
    "\n",
    "        #Get adjacency matrices from graphs\n",
    "        list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,size)\n",
    "\n",
    "        for seed in range(10):\n",
    "            \n",
    "            currentresult = {'data_train': data_train, 'seed':seed, 'copies': c}\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            print(f\"G:{g}, size: {size}, copies: {c}, seed {seed}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "            \n",
    "            hidden = 20\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "\n",
    "#            with torch.no_grad():\n",
    "#                r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "#\n",
    "#            currentresult['no_train'] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "            train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model=model,device=device,optimizer=optimizer,size=size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "            currentresult['train'] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "            if len(Results[data_test]['true']) == 0:\n",
    "                Results[data_test]['true'] = r['true']\n",
    "\n",
    "            Results[data_test]['pred'].append(currentresult)\n",
    "\n",
    "            with open(f\"SF_real_performance_{epochs}_epochs.pickle\",\"wb\") as fopen:\n",
    "                pickle.dump(Results,fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in ['LFR']:\n",
    "    \n",
    "    with open(f\"{g}_real_performance.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    for net in data:\n",
    "        \n",
    "        d = {1:[],10:[],20:[],40:[]}\n",
    "\n",
    "        for p in data[net]['pred']:\n",
    "            d[p['copies']].append(p['train']['kt'])\n",
    "        \n",
    "        for c in d.keys():\n",
    "            if len(d[c]) != 10:\n",
    "                continue\n",
    "            print(f\"Graph: {net} training: {g} Copies: {c} data: {len(d[c])}\")\n",
    "            print(round(np.mean(np.array(d[c])),4),round(np.std(np.array(d[c])),4))\n",
    "            print(d[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in ['SF']:\n",
    "    \n",
    "    with open(f\"{g}_real_performance.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    for net in data:\n",
    "        \n",
    "        d = {1:[],10:[],20:[],40:[]}\n",
    "\n",
    "        for p in data[net]['pred']:\n",
    "            d[p['copies']].append(p['train']['kt'])\n",
    "        \n",
    "        for c in d.keys():\n",
    "            if len(d[c]) != 10:\n",
    "                continue\n",
    "            print(f\"Graph: {net} training: {g} Copies: {c} data: {len(d[c])}\")\n",
    "            print(round(np.mean(np.array(d[c])),4),round(np.std(np.array(d[c])),4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generation models parallelaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "def paral_func(size,copies,seed,list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train, data_train):\n",
    "    \n",
    "    print(f\"Starting: size:{size}, copies: {copies},seed: {seed}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")    \n",
    "    torch.manual_seed(seed)           \n",
    "    hidden = 20\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "\n",
    "    epochs = 5 #epochs = 15\n",
    "    for e in range(epochs):\n",
    "        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model=model,device=device,optimizer=optimizer,size=size)\n",
    "        print(f\"Computed {data_train}_{seed}_seed_{e}_epoch, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "    \n",
    "    saving_path = f'models/{data_train}_{seed}_seed_{e}_epoch'\n",
    "    torch.save(model.state_dict(), saving_path)\n",
    "\n",
    "    print(f\"Finished {saving_path}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    graphs = ['1-wiki-Vote','2-soc-Epinions','3-email-EuAll','4-web-Google']\n",
    "    sizes = [10000,100000,300000,900000]\n",
    "\n",
    "    graphs = ['2-soc-Epinions','3-email-EuAll','4-web-Google'] #graphs = ['1-wiki-Vote']\n",
    "    sizes = [100000,300000,900000] #sizes = [10000]\n",
    "\n",
    "    for i in range(len(graphs)):\n",
    "\n",
    "        size = sizes[i]\n",
    "\n",
    "        for c in [1,10,20,40]:\n",
    "            \n",
    "            LFR_data_train = f\"LFR_5_graphs_{c}_copies_{size}_size.pickle\"\n",
    "            SF_data_train = f\"SF_5_graphs_10000_nodes_{c}_copies_{size}_size.pickle\"\n",
    "            \n",
    "            data_train = LFR_data_train\n",
    "            \n",
    "            #Load training data\n",
    "            with open(\"./data_splits/train/\"+data_train,\"rb\") as fopen:\n",
    "                list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train = pickle.load(fopen)\n",
    "\n",
    "            #Get adjacency matrices from graphs\n",
    "            list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,size)\n",
    "\n",
    "            print(f\"Starting: Size: {size}, copies: {c}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "            \n",
    "            processes = []\n",
    "            \n",
    "            seeds = 16\n",
    "\n",
    "            for batch in range(seeds//4):\n",
    "                for id in range(4):\n",
    "                    seed = id+batch*4\n",
    "                    #paral_func(size,c,seed,list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train)\n",
    "                    \n",
    "                    p = mp.Process(target=paral_func,args=[size,c,seed,list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,data_train[:-7]])\n",
    "                    p.start()\n",
    "                    processes.append(p)\n",
    "\n",
    "\n",
    "                for process in processes:\n",
    "                    process.join()\n",
    "\n",
    "            print(f\"Finished: Size: {size}, copies: {c}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generation models parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "def paral_func(size,copies,seed,list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train, data_train):\n",
    "    \n",
    "    print(f\"Starting: size:{size}, copies: {copies},seed: {seed}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")    \n",
    "    torch.manual_seed(seed)           \n",
    "    hidden = 20\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "\n",
    "    epochs = 15\n",
    "    for e in range(epochs):\n",
    "        train(list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,model=model,device=device,optimizer=optimizer,size=size)\n",
    "\n",
    "        saving_path = f'models/{data_train}_{seed}_seed_{e}_epoch'\n",
    "        torch.save(model.state_dict(), saving_path)\n",
    "    \n",
    "        print(f\"Finished {saving_path}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    graphs = ['1-wiki-Vote','2-soc-Epinions','3-email-EuAll','4-web-Google']\n",
    "    sizes = [10000,100000,300000,900000]\n",
    "\n",
    "    graphs = ['1-wiki-Vote']\n",
    "    sizes = [10000]\n",
    "\n",
    "    for i in range(len(graphs)):\n",
    "\n",
    "        size = sizes[i]\n",
    "\n",
    "        for c in [1,10,20,40]:\n",
    "            \n",
    "            LFR_data_train = f\"LFR_5_graphs_{c}_copies_{size}_size.pickle\"\n",
    "            SF_data_train = f\"SF_5_graphs_10000_nodes_{c}_copies_{size}_size.pickle\"\n",
    "            \n",
    "            data_train = SF_data_train\n",
    "            \n",
    "            #Load training data\n",
    "            with open(\"./data_splits/train/\"+data_train,\"rb\") as fopen:\n",
    "                list_graph_train,list_n_seq_train,list_num_node_train,bc_mat_train = pickle.load(fopen)\n",
    "\n",
    "            #Get adjacency matrices from graphs\n",
    "            list_adj_train,list_adj_t_train = graph_to_adj_bet(list_graph_train,list_n_seq_train,list_num_node_train,size)\n",
    "\n",
    "            print(f\"Starting: Size: {size}, copies: {c}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "            \n",
    "            processes = []\n",
    "            \n",
    "            seeds = 16\n",
    "\n",
    "            for batch in range(seeds//4):\n",
    "                for id in range(4):\n",
    "                    seed = id+batch*4\n",
    "                    #paral_func(size,c,seed,list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train)\n",
    "                    \n",
    "                    p = mp.Process(target=paral_func,args=[size,c,seed,list_adj_train,list_adj_t_train,list_num_node_train,bc_mat_train,data_train[:-7]])\n",
    "                    p.start()\n",
    "                    processes.append(p)\n",
    "\n",
    "\n",
    "                for process in processes:\n",
    "                    process.join()\n",
    "\n",
    "            print(f\"Finished: Size: {size}, copies: {c}, Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read models generate results 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "graphs = ['1-wiki-Vote','2-soc-Epinions','3-email-EuAll','4-web-Google']\n",
    "sizes = [10000,100000,300000,900000]\n",
    "\n",
    "graphs = ['2-soc-Epinions']\n",
    "sizes = [100000]\n",
    "\n",
    "\n",
    "Results = {'LFR': {}, 'SF': {}}\n",
    "\n",
    "for i in range(len(graphs)):\n",
    "\n",
    "    g = graphs[i]\n",
    "    size = sizes[i]\n",
    "    data_test = f'{g}_{size}_size.pickle'\n",
    "\n",
    "    #Load test data\n",
    "    with open(\"./data_splits/test/\"+data_test,\"rb\") as fopen:\n",
    "        list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test = pickle.load(fopen)\n",
    "\n",
    "    list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "    Results['LFR'][f\"{size}_size\"] = {'test_graph': data_test, 'real': []}\n",
    "    Results['SF'][f\"{size}_size\"] = {'test_graph': data_test, 'real': []}\n",
    "\n",
    "    for c in [1,10,20,40]:\n",
    "    \n",
    "        LFR_data_train = f\"LFR_5_graphs_{c}_copies_{size}_size.pickle\"\n",
    "        SF_data_train = f\"SF_5_graphs_10000_nodes_{c}_copies_{size}_size.pickle\"\n",
    "        \n",
    "        Results['LFR'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : LFR_data_train,'pred':{}}\n",
    "        Results['SF'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : SF_data_train,'pred':{}}\n",
    "        \n",
    "        for epoch in [4]:\n",
    "            # We analyse the results for the different networks and 5 epochs (0 to 4)\n",
    "            Results['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "            Results['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "        \n",
    "            for seed in range(15):\n",
    "                    \n",
    "                    data_train = LFR_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    print(model_path)\n",
    "                    torch.manual_seed(seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "                    if len(Results['LFR'][f\"{size}_size\"]['real']) == 0:\n",
    "                        Results['LFR'][f\"{size}_size\"]['real'] = r['true']\n",
    "\n",
    "                    with open(f\"LFR_real_performance_full_{g}_5_epochs.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n",
    "\n",
    "                    data_train = SF_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    torch.manual_seed(seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "                    if len(Results['SF'][f\"{size}_size\"]['real']) == 0:\n",
    "                        Results['SF'][f\"{size}_size\"]['real'] = r['true']\n",
    "\n",
    "                    with open(f\"SF_real_performance_full_{g}_5_epochs.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read models generate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model_bet import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "graphs = ['1-wiki-Vote','2-soc-Epinions','3-email-EuAll','4-web-Google']\n",
    "sizes = [10000,100000,300000,900000]\n",
    "\n",
    "graphs = ['1-wiki-Vote']\n",
    "sizes = [10000]\n",
    "\n",
    "Results = {'LFR': {}, 'SF': {}}\n",
    "\n",
    "for i in range(len(graphs)):\n",
    "\n",
    "    g = graphs[i]\n",
    "    size = sizes[i]\n",
    "    data_test = f'{g}_{size}_size.pickle'\n",
    "\n",
    "    #Load test data\n",
    "    with open(\"./data_splits/test/\"+data_test,\"rb\") as fopen:\n",
    "        list_graph_test,list_n_seq_test,list_num_node_test,bc_mat_test = pickle.load(fopen)\n",
    "\n",
    "    list_adj_test,list_adj_t_test = graph_to_adj_bet(list_graph_test,list_n_seq_test,list_num_node_test,size)\n",
    "\n",
    "    Results['LFR'][f\"{size}_size\"] = {'test_graph': data_test, 'real': []}\n",
    "    Results['SF'][f\"{size}_size\"] = {'test_graph': data_test, 'real': []}\n",
    "\n",
    "    for c in [1,10,20,40]:\n",
    "    \n",
    "        LFR_data_train = f\"LFR_5_graphs_{c}_copies_{size}_size.pickle\"\n",
    "        SF_data_train = f\"SF_5_graphs_10000_nodes_{c}_copies_{size}_size.pickle\"\n",
    "        \n",
    "        Results['LFR'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : LFR_data_train,'pred':{}}\n",
    "        Results['SF'][f\"{size}_size\"][f\"{c}_copies\"] = {'data_train' : SF_data_train,'pred':{}}\n",
    "        \n",
    "        for epoch in range(15):\n",
    "            \n",
    "            Results['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "            Results['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'] = {}\n",
    "        \n",
    "            for seed in range(15):\n",
    "                    \n",
    "                    data_train = LFR_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    print(model_path)\n",
    "                    torch.manual_seed(seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results['LFR'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "                    if len(Results['LFR'][f\"{size}_size\"]['real']) == 0:\n",
    "                        Results['LFR'][f\"{size}_size\"]['real'] = r['true']\n",
    "\n",
    "                    with open(f\"LFR_real_performance_full_{g}.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n",
    "\n",
    "                    data_train = SF_data_train\n",
    "                    model_path = f'{data_train[:-7]}_{seed}_seed_{epoch}_epoch'\n",
    "                    torch.manual_seed(seed)\n",
    "                    hidden = 20\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = GNN_Bet(ninput=size,nhid=hidden,dropout=0.6)\n",
    "                    model.load_state_dict(torch.load(f'models/{model_path}'))\n",
    "                    model.to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "                    with torch.no_grad():\n",
    "                        r = test_onegraph(list_adj_test,list_adj_t_test,list_num_node_test,bc_mat_test,model=model,device=device,size=size)\n",
    "\n",
    "                    Results['SF'][f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"] = {'pred':r['pred'],'kt':r[\"kt\"]}\n",
    "\n",
    "                    if Results['SF'][f\"{size}_size\"]['real'] == -1:\n",
    "                        Results['SF'][f\"{size}_size\"]['real'] = r['true']\n",
    "\n",
    "                    with open(f\"SF_real_performance_full_{g}.pickle\",\"wb\") as fopen:\n",
    "                        pickle.dump(Results,fopen)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read  generated full results 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realgraph = '1-wiki-Vote'\n",
    "size = 10000\n",
    "epoch = 4\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(epoch)\n",
    "    \n",
    "    for graphtype in ['LFR','SF']:\n",
    "\n",
    "        with open(f\"{graphtype}_real_performance_full_{realgraph}.pickle\",'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        line = graphtype+' '+realgraph\n",
    "        for copies in [1,10,20,40]:\n",
    "            aux = []\n",
    "            for seed in range(10):\n",
    "                aux.append(data[graphtype][f'{size}_size'][f'{copies}_copies']['pred'][f'{epoch}_epoch'][f'{seed}_seed']['kt'])\n",
    "            line += f' & {round(np.mean(np.array(aux)),4)} \\pm {round(np.std(np.array(aux)),4)}'\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realgraph = '2-soc-Epinions'\n",
    "size = 100000\n",
    "epoch = 4\n",
    "\n",
    "for graphtype in ['LFR','SF']:\n",
    "\n",
    "    with open(f\"{graphtype}_real_performance_full_{realgraph}_5_epochs.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    line = graphtype+' '+realgraph\n",
    "    for copies in [1,10,20,40]:\n",
    "        aux = []\n",
    "        for seed in range(10):\n",
    "            aux.append(data[graphtype][f'{size}_size'][f'{copies}_copies']['pred'][f'{epoch}_epoch'][f'{seed}_seed']['kt'])\n",
    "        line += f' & {round(np.mean(np.array(aux)),4)} \\pm {round(np.std(np.array(aux)),4)}'\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read generatedfull results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphtype = 'LFR'\n",
    "realgraph = '1-wiki-Vote'\n",
    "size = 10000\n",
    "\n",
    "with open(f\"{graphtype}_real_performance_full_{realgraph}.pickle\",'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data['LFR']\n",
    "true = data[f\"{size}_size\"]['real']\n",
    "\n",
    "plotting_data = {}\n",
    "\n",
    "for c in [1,10,20,40]:\n",
    "    \n",
    "    plotting_data[c] = {'xs':[], 'ys':[],'err':[]}\n",
    "\n",
    "    for epoch in range(15):\n",
    "\n",
    "        plotting_data[c]['xs'].append(epoch)\n",
    "        \n",
    "        aux = []\n",
    "        \n",
    "        for seed in range(15): #range(15):\n",
    "\n",
    "            # {'pred':r['pred'],'kt':r[\"kt\"]} -> schema\n",
    "            pred = data[f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"]\n",
    "            aux.append(pred['kt'])\n",
    "            \n",
    "        plotting_data[c]['ys'].append(round(np.mean(np.array(aux)),4))\n",
    "        plotting_data[c]['err'].append(round(np.std(np.array(aux)),4))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for c in plotting_data:\n",
    "    xs = [j+1 for j in plotting_data[c]['xs']]\n",
    "    ys = plotting_data[c]['ys']\n",
    "    err = plotting_data[c]['err']\n",
    "    plt.errorbar(xs[:],ys[:],err[:])\n",
    "    plt.scatter(xs[:],ys[:])\n",
    "    plt.plot(xs,ys,label=f'Copies: {c}')\n",
    "\n",
    "plt.title(f\"{realgraph} trained with {graphtype} graphs\")\n",
    "plt.xlabel(\"Number of training epochs\")\n",
    "plt.ylabel(\"KT Score\")\n",
    "plt.ylim(0.7,1)\n",
    "plt.xticks(range(1,16),xs)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphtype = 'SF'\n",
    "realgraph = '1-wiki-Vote'\n",
    "size = 10000\n",
    "\n",
    "with open(f\"{graphtype}_real_performance_full_{realgraph}.pickle\",'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data['SF']\n",
    "true = data[f\"{size}_size\"]['real']\n",
    "\n",
    "plotting_data = {}\n",
    "\n",
    "for c in [1,10,20,40]:\n",
    "    \n",
    "    plotting_data[c] = {'xs':[], 'ys':[]}\n",
    "\n",
    "    for epoch in range(15):\n",
    "\n",
    "        plotting_data[c]['xs'].append(epoch)\n",
    "        \n",
    "        aux = []\n",
    "        \n",
    "        for seed in range(15): #range(15):\n",
    "\n",
    "            # {'pred':r['pred'],'kt':r[\"kt\"]} -> schema\n",
    "            pred = data[f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"]\n",
    "            aux.append(pred['kt'])\n",
    "            \n",
    "        plotting_data[c]['ys'].append(round(np.mean(np.array(aux)),4))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for c in plotting_data:\n",
    "    xs = [j+1 for j in plotting_data[c]['xs']]\n",
    "    ys = plotting_data[c]['ys']\n",
    "    plt.plot(xs,ys,label=f'Copies: {c}')\n",
    "\n",
    "plt.title(f\"{realgraph} trained with {graphtype} graphs\")\n",
    "plt.xlabel(\"Number of training epochs\")\n",
    "plt.ylabel(\"KT Score\")\n",
    "plt.ylim(0.7,1)\n",
    "plt.xticks(range(1,16),xs)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphtype = 'LFR'\n",
    "realgraph = '1-wiki-Vote'\n",
    "size = 10000\n",
    "\n",
    "with open(f\"{graphtype}_real_performance_full_{realgraph}.pickle\",'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data['LFR']\n",
    "true = data[f\"{size}_size\"]['real']\n",
    "\n",
    "plotting_data = {}\n",
    "\n",
    "for c in [10]:# [1,10,20,40]:\n",
    "    \n",
    "    plotting_data[c] = {'xs':[], 'ys':[],'err':[]}\n",
    "\n",
    "    for epoch in range(15):\n",
    "\n",
    "        plotting_data[c]['xs'].append(epoch)\n",
    "        \n",
    "        aux = []\n",
    "        \n",
    "        for seed in range(15): #range(15):\n",
    "\n",
    "            # {'pred':r['pred'],'kt':r[\"kt\"]} -> schema\n",
    "            pred = data[f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"]\n",
    "            aux.append(pred['kt'])\n",
    "            \n",
    "        plotting_data[c]['ys'].append(round(np.mean(np.array(aux)),4))\n",
    "        plotting_data[c]['err'].append(round(np.std(np.array(aux)),4))\n",
    "\n",
    "\n",
    "for c in plotting_data:\n",
    "    xs = [j+1 for j in plotting_data[c]['xs']]\n",
    "    ys = plotting_data[c]['ys']\n",
    "    plt.plot(xs,ys,label=f'{graphtype} {c} copies')\n",
    "\n",
    "for c in plotting_data:\n",
    "    \n",
    "    xs = [j+1 for j in plotting_data[c]['xs']]\n",
    "    ys = plotting_data[c]['ys']\n",
    "    err = plotting_data[c]['err']\n",
    "    #plt.errorbar(xs[:],ys[:],err[:],c='lightskyblue')\n",
    "    #plt.scatter(xs[:],ys[:],c='lightskyblue')\n",
    "    plt.plot(xs,ys,c='lightskyblue',label=f'Copies: {c}')\n",
    "\n",
    "graphtype = 'SF'\n",
    "realgraph = '1-wiki-Vote'\n",
    "size = 10000\n",
    "\n",
    "with open(f\"{graphtype}_real_performance_full_{realgraph}.pickle\",'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data['SF']\n",
    "true = data[f\"{size}_size\"]['real']\n",
    "\n",
    "plotting_data = {}\n",
    "\n",
    "for c in [10]:# [1,10,20,40]:\n",
    "    \n",
    "    plotting_data[c] = {'xs':[], 'ys':[],'err': []}\n",
    "\n",
    "    for epoch in range(15):\n",
    "\n",
    "        plotting_data[c]['xs'].append(epoch)\n",
    "        \n",
    "        aux = []\n",
    "        \n",
    "        for seed in range(15): #range(15):\n",
    "\n",
    "            # {'pred':r['pred'],'kt':r[\"kt\"]} -> schema\n",
    "            pred = data[f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"]\n",
    "            aux.append(pred['kt'])\n",
    "            \n",
    "        plotting_data[c]['ys'].append(round(np.mean(np.array(aux)),4))\n",
    "        plotting_data[c]['err'].append(round(np.std(np.array(aux)),4))\n",
    "\n",
    "for c in plotting_data:\n",
    "    xs = [j+1 for j in plotting_data[c]['xs']]\n",
    "    ys = plotting_data[c]['ys']\n",
    "    err = plotting_data[c]['err']\n",
    "    #plt.errorbar(xs[:],ys[:],err[:],c='lightcoral')\n",
    "    #plt.scatter(xs[:],ys[:],c='lightcoral')\n",
    "    #plt.plot(xs,ys,c='lightcoral',label=f'Copies: {c}')\n",
    "\n",
    "\n",
    "plt.title(f\"{realgraph} trained with {graphtype} graphs\")\n",
    "plt.xlabel(\"Number of training epochs\")\n",
    "plt.ylabel(\"KT Score\")\n",
    "\n",
    "plt.xticks(range(1,16),xs)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "graphtype = 'LFR'\n",
    "realgraph = '1-wiki-Vote'\n",
    "size = 10000\n",
    "\n",
    "with open(f\"{graphtype}_real_performance_full_{realgraph}.pickle\",'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data['LFR']\n",
    "true = data[f\"{size}_size\"]['real']\n",
    "\n",
    "plotting_data = {}\n",
    "\n",
    "for c in [40]:# [1,10,20,40]:\n",
    "    \n",
    "    plotting_data[c] = {'xs':[], 'ys':[],'err':[]}\n",
    "\n",
    "    for epoch in range(15):\n",
    "\n",
    "        plotting_data[c]['xs'].append(epoch)\n",
    "        \n",
    "        aux = []\n",
    "        \n",
    "        for seed in range(15): #range(15):\n",
    "\n",
    "            # {'pred':r['pred'],'kt':r[\"kt\"]} -> schema\n",
    "            pred = data[f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"]\n",
    "            aux.append(pred['kt'])\n",
    "            \n",
    "        plotting_data[c]['ys'].append(round(np.mean(np.array(aux)),4))\n",
    "        plotting_data[c]['err'].append(round(np.std(np.array(aux)),4))\n",
    "\n",
    "\n",
    "for c in plotting_data:\n",
    "    \n",
    "    xs = [j+1 for j in plotting_data[c]['xs']]\n",
    "    ys = plotting_data[c]['ys']\n",
    "    err = plotting_data[c]['err']\n",
    "    plt.errorbar(xs[:],ys[:],err[:],c='b')\n",
    "    plt.scatter(xs[:],ys[:],c='b')\n",
    "    plt.plot(xs,ys,c='b',label=f'LFR, copies: {c}')\n",
    "\n",
    "graphtype = 'SF'\n",
    "realgraph = '1-wiki-Vote'\n",
    "size = 10000\n",
    "\n",
    "with open(f\"{graphtype}_real_performance_full_{realgraph}.pickle\",'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data['SF']\n",
    "true = data[f\"{size}_size\"]['real']\n",
    "\n",
    "plotting_data = {}\n",
    "\n",
    "for c in [40]:# [1,10,20,40]:\n",
    "    \n",
    "    plotting_data[c] = {'xs':[], 'ys':[],'err': []}\n",
    "\n",
    "    for epoch in range(15):\n",
    "\n",
    "        plotting_data[c]['xs'].append(epoch)\n",
    "        \n",
    "        aux = []\n",
    "        \n",
    "        for seed in range(15): #range(15):\n",
    "\n",
    "            # {'pred':r['pred'],'kt':r[\"kt\"]} -> schema\n",
    "            pred = data[f\"{size}_size\"][f\"{c}_copies\"]['pred'][f'{epoch}_epoch'][f\"{seed}_seed\"]\n",
    "            aux.append(pred['kt'])\n",
    "            \n",
    "        plotting_data[c]['ys'].append(round(np.mean(np.array(aux)),4))\n",
    "        plotting_data[c]['err'].append(round(np.std(np.array(aux)),4))\n",
    "\n",
    "for c in plotting_data:\n",
    "    xs = [j+1 for j in plotting_data[c]['xs']]\n",
    "    ys = plotting_data[c]['ys']\n",
    "    err = plotting_data[c]['err']\n",
    "    plt.errorbar(xs[:],ys[:],err[:],c='darkred')\n",
    "    plt.scatter(xs[:],ys[:],c='darkred')\n",
    "    plt.plot(xs,ys,c='darkred',label=f'SF, copies: {c}')\n",
    "\n",
    "\n",
    "plt.title(f\"{realgraph} LFR vs SF trained graphs\")\n",
    "plt.xlabel(\"Number of training epochs\")\n",
    "plt.ylabel(\"KT Score\")\n",
    "\n",
    "plt.xticks(range(1,16),xs)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
